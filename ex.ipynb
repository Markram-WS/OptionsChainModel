{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1070256-4ec0-4b13-a32b-4a9c3953f205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2023.9.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (10.3.0)\n",
      "Downloading rdkit-2023.9.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2023.9.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rdkit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82ae179-3ae4-4404-bd55-24270f0d5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from IPython.display import clear_output,display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c46a667-f9f9-44d0-9c4f-f9482e4ebf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import BondType\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da564aa-4602-4c8f-b5b2-b46ded791555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/250k_rndm_zinc_drugs_clean_3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/250k_rndm_zinc_drugs_clean_3.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#df = pd.read_csv(\"/content/250k_rndm_zinc_drugs_clean_3.csv\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiles\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/file_utils.py:291\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m         \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n",
      "File \u001b[0;32m/usr/lib/python3.11/urllib/request.py:251\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Handle temporary file setup.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m--> 251\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/250k_rndm_zinc_drugs_clean_3.csv'"
     ]
    }
   ],
   "source": [
    "csv_path = keras.utils.get_file(\n",
    "    \"/content/250k_rndm_zinc_drugs_clean_3.csv\",\n",
    "    \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\",\n",
    ")\n",
    "\n",
    "#df = pd.read_csv(\"/content/250k_rndm_zinc_drugs_clean_3.csv\")\n",
    "df[\"smiles\"] = df[\"smiles\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4df41d25-8212-4a54-aee7-236b18dfdc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILE_CHARSET = '[\"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \"P\", \"Cl\", \"Br\"]'\n",
    "\n",
    "bond_mapping = {\"SINGLE\": 0, \"DOUBLE\": 1, \"TRIPLE\": 2, \"AROMATIC\": 3}\n",
    "bond_mapping.update(\n",
    "    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}\n",
    ")\n",
    "SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n",
    "\n",
    "MAX_MOLSIZE = 36 #max(df[\"smiles\"].str.len())\n",
    "SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\n",
    "index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\n",
    "atom_mapping = dict(SMILE_to_index)\n",
    "atom_mapping.update(index_to_SMILE)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "\n",
    "VAE_LR = 5e-4\n",
    "NUM_ATOMS = 120  # Maximum number of atoms\n",
    "\n",
    "ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\n",
    "BOND_DIM = 4 + 1  # Number of bond types\n",
    "LATENT_DIM = 435  # Size of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35179160-5950-4133-839d-734baf6aaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = df.sample(frac=0.75, random_state=42)  # random state is a seed value\n",
    "# train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# adjacency_tensor, feature_tensor, qed_tensor = [], [], []\n",
    "# for idx in range(8000):\n",
    "#     adjacency, features = smiles_to_graph(train_df.loc[idx][\"smiles\"])\n",
    "#     qed = train_df.loc[idx][\"qed\"]\n",
    "#     adjacency_tensor.append(adjacency)\n",
    "#     feature_tensor.append(features)\n",
    "#     qed_tensor.append(qed)\n",
    "\n",
    "# adjacency_tensor = np.array(adjacency_tensor)\n",
    "# feature_tensor = np.array(feature_tensor)\n",
    "# qed_tensor = np.array(qed_tensor)\n",
    "\n",
    "\n",
    "class RelationalGraphConvLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units=128,\n",
    "        activation=\"relu\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        bond_dim = input_shape[0][1]\n",
    "        atom_dim = input_shape[1][2]\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(bond_dim, atom_dim, self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            trainable=True,\n",
    "            name=\"W\",\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(bond_dim, 1, self.units),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                trainable=True,\n",
    "                name=\"b\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        adjacency, features = inputs\n",
    "        # Aggregate information from neighbors\n",
    "        x = tf.matmul(adjacency, features[:, None, :, :])\n",
    "        # Apply linear transformation\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            x += self.bias\n",
    "        # Reduce bond types dim\n",
    "        x_reduced = tf.reduce_sum(x, axis=1)\n",
    "        # Apply non-linear transformation\n",
    "        return self.activation(x_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d176e2e2-85c8-4c1f-9026-13fd007dd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(\n",
    "    gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate\n",
    "):\n",
    "    adjacency = keras.layers.Input(shape=adjacency_shape)\n",
    "    features = keras.layers.Input(shape=feature_shape)\n",
    "\n",
    "    # Propagate through one or more graph convolutional layers\n",
    "    features_transformed = features\n",
    "    print(features_transformed)\n",
    "    for units in gconv_units:\n",
    "        features_transformed = RelationalGraphConvLayer(units)(\n",
    "            [adjacency, features_transformed]\n",
    "        )\n",
    "    # Reduce 2-D representation of molecule to 1-D\n",
    "    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n",
    "\n",
    "    # Propagate through one or more densely connected layers\n",
    "    for units in dense_units:\n",
    "        x = layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, dtype=\"float32\", name=\"z_mean\")(x)\n",
    "    log_var = layers.Dense(latent_dim, dtype=\"float32\", name=\"log_var\")(x)\n",
    "\n",
    "    encoder = keras.Model([adjacency, features], [z_mean, log_var], name=\"encoder\")\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "    x = latent_inputs\n",
    "    for units in dense_units:\n",
    "        x = keras.layers.Dense(units, activation=\"tanh\")(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
    "    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)\n",
    "    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n",
    "    # Symmetrify tensors in the last two dimensions\n",
    "    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
    "    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
    "    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)\n",
    "    x_features = keras.layers.Reshape(feature_shape)(x_features)\n",
    "    x_features = keras.layers.Softmax(axis=2)(x_features)\n",
    "\n",
    "    decoder = keras.Model(\n",
    "        latent_inputs, outputs=[x_adjacency, x_features], name=\"decoder\"\n",
    "    )\n",
    "\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc502e68-50d3-41d6-9441-ae67c66a7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_log_var)[0]\n",
    "        dim = tf.shape(z_log_var)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb9825b6-991c-4697-8b66-f6110763d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeGenerator(keras.Model):\n",
    "    def __init__(self, encoder, decoder, max_len, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.property_prediction_layer = layers.Dense(1)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.train_total_loss_tracker = keras.metrics.Mean(name=\"train_total_loss\")\n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
    "\n",
    "    def train_step(self, data):\n",
    "        adjacency_tensor, feature_tensor, qed_tensor = data[0]\n",
    "        graph_real = [adjacency_tensor, feature_tensor]\n",
    "        self.batch_size = tf.shape(qed_tensor)[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, qed_pred, gen_adjacency, gen_features = self(\n",
    "                graph_real, training=True\n",
    "            )\n",
    "            graph_generated = [gen_adjacency, gen_features]\n",
    "            total_loss = self._compute_loss(\n",
    "                z_log_var, z_mean, qed_tensor, qed_pred, graph_real, graph_generated\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.train_total_loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.train_total_loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(\n",
    "        self, z_log_var, z_mean, qed_true, qed_pred, graph_real, graph_generated\n",
    "    ):\n",
    "\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_gen, features_gen = graph_generated\n",
    "\n",
    "        adjacency_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.categorical_crossentropy(adjacency_real, adjacency_gen),\n",
    "                axis=(1, 2),\n",
    "            )\n",
    "        )\n",
    "        features_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.categorical_crossentropy(features_real, features_gen),\n",
    "                axis=(1),\n",
    "            )\n",
    "        )\n",
    "        kl_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1\n",
    "        )\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "        property_loss = tf.reduce_mean(\n",
    "            keras.losses.binary_crossentropy(qed_true, qed_pred)\n",
    "        )\n",
    "\n",
    "        graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
    "\n",
    "        return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss\n",
    "\n",
    "    def _gradient_penalty(self, graph_real, graph_generated):\n",
    "        # Unpack graphs\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_generated, features_generated = graph_generated\n",
    "\n",
    "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
    "        alpha = tf.random.uniform([self.batch_size])\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
    "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n",
    "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
    "\n",
    "        # Compute the logits of interpolated graphs\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adjacency_interp)\n",
    "            tape.watch(features_interp)\n",
    "            _, _, logits, _, _ = self(\n",
    "                [adjacency_interp, features_interp], training=True\n",
    "            )\n",
    "\n",
    "        # Compute the gradients with respect to the interpolated graphs\n",
    "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
    "        # Compute the gradient penalty\n",
    "        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n",
    "        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n",
    "        return tf.reduce_mean(\n",
    "            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n",
    "            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n",
    "        )\n",
    "\n",
    "    def inference(self, batch_size):\n",
    "        z = tf.random.normal((batch_size, LATENT_DIM))\n",
    "        reconstruction_adjacency, reconstruction_features = model.decoder.predict(z)\n",
    "        # obtain one-hot encoded adjacency tensor\n",
    "        adjacency = tf.argmax(reconstruction_adjacency, axis=1)\n",
    "        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
    "        # Remove potential self-loops from adjacency\n",
    "        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
    "        # obtain one-hot encoded feature tensor\n",
    "        features = tf.argmax(reconstruction_features, axis=2)\n",
    "        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
    "        return [\n",
    "            graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, log_var = self.encoder(inputs)\n",
    "        z = Sampling()([z_mean, log_var])\n",
    "\n",
    "        gen_adjacency, gen_features = self.decoder(z)\n",
    "\n",
    "        property_pred = self.property_prediction_layer(z_mean)\n",
    "\n",
    "        return z_mean, log_var, property_pred, gen_adjacency, gen_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f273996b-af57-46ef-a2c1-cadbd3466fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KerasTensor shape=(None, 120, 11), dtype=float32, sparse=None, name=keras_tensor_55>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: <property object at 0x7f405ed4d170>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m vae_optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mVAE_LR)\n\u001b[1;32m      3\u001b[0m encoder \u001b[38;5;241m=\u001b[39m get_encoder(\n\u001b[1;32m      4\u001b[0m     gconv_units\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m9\u001b[39m],\n\u001b[1;32m      5\u001b[0m     adjacency_shape\u001b[38;5;241m=\u001b[39m(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m decoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLATENT_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43madjacency_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBOND_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ATOMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ATOMS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUM_ATOMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mATOM_DIM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#model.compile(vae_optimizer)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 39\u001b[0m, in \u001b[0;36mget_decoder\u001b[0;34m(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape)\u001b[0m\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(dropout_rate)(x)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m x_adjacency \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43madjacency_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m x_adjacency \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mReshape(adjacency_shape)(x_adjacency)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Symmetrify tensors in the last two dimensions\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py:397\u001b[0m, in \u001b[0;36mstandardize_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ALLOWED_DTYPES:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid dtype: <property object at 0x7f405ed4d170>"
     ]
    }
   ],
   "source": [
    "vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)\n",
    "\n",
    "encoder = get_encoder(\n",
    "    gconv_units=[9],\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    "    latent_dim=LATENT_DIM,\n",
    "    dense_units=[512],\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "decoder = get_decoder(\n",
    "    dense_units=[128, 256, 512],\n",
    "    dropout_rate=0.2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "\n",
    "# model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\n",
    "\n",
    "#model.compile(vae_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b336e9b5-a13a-46a5-a3fe-30bc03092aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Tensor(1, shape=(), dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f373e250-b7e5-4876-9188-b1b0dc3fdc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATOM_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7b814-8506-421c-a16b-d557ce3f4781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
