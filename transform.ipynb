{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be options chain day1 - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import joblib\n",
    "import random\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH     = './data/OptionsEOD.csv/'\n",
    "#create @ part 1\n",
    "PARQUET_PATH = './data/OptionsEOD.parquet'\n",
    "#create @ part 2\n",
    "PARQUET_STG_PATH = './data/OptionsEOD_STG.parquet'\n",
    "SCALER_COL  = ['UNDERLYING_LAST','STRIKE','STRIKE_DISTANCE','INTRINSIC_VALUE','DTE','TOTAL_VOLUME','C_VEGA','P_VEGA',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK', 'P_VOLUME' ]\n",
    "#create @ part 3\n",
    "ID = ['OPTIONS_ID']\n",
    "SCALER_PATH = ['./data/scaler/' , 'scaler.gz']\n",
    "UNIQUE_KEYS = ['QUOTE_DATE','SYMBOL','EXPIRE_DATE']\n",
    "H5_PATH = './data/OptTrainData/'\n",
    "START = True#True#'2010-09'#True\n",
    "\n",
    "\n",
    "max_option_len = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== Create options_qoute\n",
    "options_qoute = {}\n",
    "#==== Create MinMaxScaler\n",
    "if not os.path.exists(SCALER_PATH[0]):\n",
    "    os.makedirs(SCALER_PATH[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[QUOTE_UNIXTIME]</th>\n",
       "      <th>[QUOTE_READTIME]</th>\n",
       "      <th>[QUOTE_DATE]</th>\n",
       "      <th>[QUOTE_TIME_HOURS]</th>\n",
       "      <th>[UNDERLYING_LAST]</th>\n",
       "      <th>[EXPIRE_DATE]</th>\n",
       "      <th>[EXPIRE_UNIX]</th>\n",
       "      <th>[DTE]</th>\n",
       "      <th>[C_DELTA]</th>\n",
       "      <th>[C_GAMMA]</th>\n",
       "      <th>...</th>\n",
       "      <th>[P_LAST]</th>\n",
       "      <th>[P_DELTA]</th>\n",
       "      <th>[P_GAMMA]</th>\n",
       "      <th>[P_VEGA]</th>\n",
       "      <th>[P_THETA]</th>\n",
       "      <th>[P_RHO]</th>\n",
       "      <th>[P_IV]</th>\n",
       "      <th>[P_VOLUME]</th>\n",
       "      <th>[STRIKE_DISTANCE]</th>\n",
       "      <th>[STRIKE_DISTANCE_PCT]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003130</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793590</td>\n",
       "      <td></td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003580</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>-0.004420</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>0.721270</td>\n",
       "      <td></td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004040</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>-0.004350</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>0.648640</td>\n",
       "      <td></td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.004390</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.577770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1325624400</td>\n",
       "      <td>2012-01-03 16:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.9</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1325883600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.971540</td>\n",
       "      <td>0.016360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.004860</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>-0.004140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   [QUOTE_UNIXTIME]   [QUOTE_READTIME]  [QUOTE_DATE]   [QUOTE_TIME_HOURS]  \\\n",
       "0        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "1        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "2        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "3        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "4        1325624400   2012-01-03 16:00    2012-01-03                 16.0   \n",
       "\n",
       "    [UNDERLYING_LAST]  [EXPIRE_DATE]   [EXPIRE_UNIX]   [DTE]  [C_DELTA]  \\\n",
       "0                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "1                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "2                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "3                56.9     2012-01-06      1325883600     3.0   1.000000   \n",
       "4                56.9     2012-01-06      1325883600     3.0   0.971540   \n",
       "\n",
       "   [C_GAMMA]  ...   [P_LAST]   [P_DELTA]  [P_GAMMA]   [P_VEGA]   [P_THETA]  \\\n",
       "0   0.000000  ...   0.000000   -0.003130   0.002320   0.000350   -0.004080   \n",
       "1   0.000000  ...   0.000000   -0.003580   0.002780   0.000530   -0.004420   \n",
       "2   0.000000  ...   0.000000   -0.004040   0.003340   0.000950   -0.004350   \n",
       "3   0.000000  ...   0.010000   -0.004390   0.004150   0.001170   -0.004050   \n",
       "4   0.016360  ...   0.010000   -0.004860   0.005270   0.001370   -0.004140   \n",
       "\n",
       "      [P_RHO]     [P_IV]  [P_VOLUME]  [STRIKE_DISTANCE]  \\\n",
       "0    0.000000   0.793590                           10.9   \n",
       "1   -0.000420   0.721270                            9.9   \n",
       "2   -0.000430   0.648640                            8.9   \n",
       "3   -0.000330   0.577770    0.000000                7.9   \n",
       "4    0.000000   0.507560    0.000000                6.9   \n",
       "\n",
       "    [STRIKE_DISTANCE_PCT]  \n",
       "0                   0.192  \n",
       "1                   0.174  \n",
       "2                   0.156  \n",
       "3                   0.139  \n",
       "4                   0.121  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "EOD_CSV = pd.read_csv(CSV_PATH+\"qqq/qqq_eod_201201.txt\", engine='pyarrow')\n",
    "EOD_CSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==================================Create OptionsEOD.parquet part 1========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part I\n",
    "#TransformData : \n",
    "#-each partition from EXPIRE_DATE \n",
    "#-csv too parquet\n",
    "#-col. rename \n",
    "def TransformDataI():\n",
    "    #scaler = MinMaxScaler()\n",
    "    scaler = StandardScaler()\n",
    "    schema = None\n",
    "    pqwriter = None\n",
    "    for d in os.listdir(CSV_PATH):\n",
    "        for f in os.listdir(CSV_PATH+f\"{d}/\"):\n",
    "            if f.endswith(\".txt\"):\n",
    "                ## load\n",
    "                print( f\"[LOAD] : {CSV_PATH}{d}/{f}        \",end='\\r')\n",
    "                EOD_CSV = pd.read_csv(CSV_PATH+f\"{d}/\"+f, engine='pyarrow')\n",
    "                    \n",
    "                ## rename col.\n",
    "                for c in EOD_CSV.columns:\n",
    "                    EOD_CSV = EOD_CSV.rename( columns={ c:c.strip().replace(']','').replace('[','') } )\n",
    "                \n",
    "                ## add symbol \n",
    "                EOD_CSV['SYMBOL'] = d.upper()\n",
    "                ## add INTRINSIC_VALUE\n",
    "                EOD_CSV['INTRINSIC_VALUE'] = EOD_CSV['UNDERLYING_LAST'] - EOD_CSV['STRIKE']\n",
    "                \n",
    "                ## fillnafillna\n",
    "                EOD_CSV['P_VOLUME'] = EOD_CSV['P_VOLUME'].fillna(0)\n",
    "                EOD_CSV['C_VOLUME'] = EOD_CSV['C_VOLUME'].fillna(0)\n",
    "                EOD_CSV.dropna(subset=[SCALER_COL])\n",
    "            \n",
    "                \n",
    "                # date columns convert to datetime\n",
    "                for c in [\"QUOTE_READTIME\",\"QUOTE_DATE\",\"EXPIRE_DATE\"]:\n",
    "                    EOD_CSV[c] = pd.to_datetime(EOD_CSV[c])\n",
    "                \n",
    "                #clean float data\n",
    "                for c in ['INTRINSIC_VALUE','C_DELTA','C_GAMMA','C_VEGA','C_THETA','C_RHO','C_IV','C_VOLUME','C_LAST','C_BID','C_ASK','STRIKE','P_BID','P_ASK','P_LAST','P_DELTA','P_GAMMA','P_VEGA','P_THETA','P_RHO','P_IV','P_VOLUME','STRIKE_DISTANCE','STRIKE_DISTANCE_PCT']:\n",
    "                    if EOD_CSV[c].dtype not in ( 'float32','float64'):\n",
    "                        EOD_CSV[c] = EOD_CSV[c].apply(lambda x: x.strip())\n",
    "                        EOD_CSV[c] = EOD_CSV[c].replace('', np.nan).fillna(np.nan)\n",
    "                        EOD_CSV[c] = EOD_CSV[c].astype('float64')\n",
    "                    if EOD_CSV[c].dtype == 'float32':\n",
    "                        EOD_CSV[c] = EOD_CSV[c].astype('float64')\n",
    "                        \n",
    "                # REMAIN_DAYS(int) =>  use DTE col.\n",
    "                #partition with QUOTE_DATE\n",
    "                EOD_CSV['PartitionDate'] = EOD_CSV['QUOTE_DATE'].dt.strftime('%Y-%m')\n",
    "                EOD_CSV.sort_values(['QUOTE_DATE','EXPIRE_DATE','SYMBOL','STRIKE'],ascending =False ) \n",
    "\n",
    "                #scaler(Normalization_\n",
    "                #scaler.partial_fit(EOD_CSV[SCALER_COL])\n",
    "\n",
    "                # save\n",
    "                if os.path.exists(PARQUET_PATH):\n",
    "                  EOD_CSV.to_parquet(PARQUET_PATH, engine='fastparquet', append=True, partition_cols=['PartitionDate'], index=False )\n",
    "                else:\n",
    "                  EOD_CSV.to_parquet(PARQUET_PATH, engine='fastparquet' , partition_cols=['PartitionDate'], index=False  )\n",
    "                    \n",
    "    # joblib.dump(scaler, SCALER_PATH )\n",
    "    # if pqwriter:\n",
    "    #     pqwriter.close()\n",
    "    # print( f\"[DONE]                                                       \",end='\\r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-RunCleanData\n",
    "#TransformDataI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==================================Create OptTrainData STG part 2========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Last Processing] 2011-12, 100.0%       \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Part II \n",
    "#START = '2012-02'\n",
    "#TransformData : \n",
    "# - read each partitions \n",
    "# - Normalization if not have scaler.gz file\n",
    "###===== load Scaler\n",
    "if START == True:\n",
    "    Scaler = StandardScaler()\n",
    "else:\n",
    "    Scaler = joblib.load(''.join(SCALER_PATH ) )\n",
    "###===== local functions\n",
    "def strikeZero(df,v,num_rm):\n",
    "    # First, filter based on QUOTE_DATE, SYMBOL, and EXPIRE_DATE\n",
    "    filtered_arr = df[(df['QUOTE_DATE'] == v['QUOTE_DATE']) &\n",
    "                     (df['SYMBOL'] == v['SYMBOL']) &\n",
    "                     (df['EXPIRE_DATE'] == v['EXPIRE_DATE'])]['STRIKE'].values\n",
    "    if len(filtered_arr) != max_option_len:\n",
    "        pass\n",
    "    # print(max_intrinsic_value)\n",
    "    # print(min_intrinsic_value)\n",
    "    df.loc[ (df['QUOTE_DATE'] == v['QUOTE_DATE']) \n",
    "            & (df['SYMBOL'] == v['SYMBOL']) \n",
    "            & (df['EXPIRE_DATE'] == v['EXPIRE_DATE'])\n",
    "            & (  (df['STRIKE'].isin(filtered_arr[:5])  )\n",
    "               | (df['STRIKE'].isin(filtered_arr[-5:])   )\n",
    "              ) \n",
    "        , SCALER_COL    \n",
    "    ] = 1e-8\n",
    "    \n",
    "def hash_str(S):\n",
    "    return hashlib.md5(S.encode('utf-8')).hexdigest() \n",
    "    \n",
    "#========================================= main model =========================================\n",
    "keys = None#df[unique_keys].sort_values(by=unique_keys).drop_duplicates()\n",
    "\n",
    "#==== Get PartitionDate\n",
    "PartitionDate = [ d[-7:] for d in  os.listdir(PARQUET_PATH) if 'PartitionDate' in d]\n",
    "#PartitionDate = ['2011-12','2022-05'] # debug\n",
    "for i,partdate in enumerate(PartitionDate[:]) :  \n",
    "    START = True if START == partdate else START\n",
    "    if START == True :\n",
    "        df = pd.read_parquet(PARQUET_PATH,engine='pyarrow'\n",
    "                                     , filters=[('PartitionDate', '=', partdate)]\n",
    "                                    )\n",
    "        df['PartitionDate'] = df['PartitionDate'].astype('object') \n",
    "        #add col options_id\n",
    "        df['P_VOLUME'] = df['P_VOLUME'].fillna(0)\n",
    "        df['C_VOLUME'] = df['C_VOLUME'].fillna(0)\n",
    "        df[df == 0] = 1e-8\n",
    "        df = df.dropna(subset=[c for c in SCALER_COL if c != 'TOTAL_VOLUME' ])\n",
    "        ####################################################\n",
    "        keys = df[UNIQUE_KEYS].sort_values(by=UNIQUE_KEYS).drop_duplicates()[:]\n",
    "        DATA  = np.empty((0,) + ( max_option_len,len(SCALER_COL) ) ) #init \n",
    "        #loop each UNIQUE_KEYS keys \n",
    "        for j,v in keys.iterrows():\n",
    "            msg = {}\n",
    "            msg['qd_ate'] = v['QUOTE_DATE']\n",
    "            msg['symbo'] = v['SYMBOL']\n",
    "            msg['exdate'] = v['EXPIRE_DATE']\n",
    "            \n",
    "            df_filter=df[ (df['QUOTE_DATE'] == v['QUOTE_DATE']) & (df['SYMBOL'] == v['SYMBOL']) & (df['EXPIRE_DATE'] == v['EXPIRE_DATE']) ]\n",
    "            msg['start_rows'] = len(df_filter) \n",
    "\n",
    "            #break loops\n",
    "            if len(df_filter) < 5 : \n",
    "                print(f\"\"\"{msg['qd_ate']}|{msg['symbo']}|{msg['exdate']} : {msg['start_rows']}->break (less rows)\"\"\")\n",
    "                break \n",
    "\n",
    "            qoute = \"\".join(v[ ['SYMBOL','EXPIRE_DATE'] ].apply(str).values)\n",
    "            #add new qoute\n",
    "            if qoute not in [*options_qoute.keys()]:\n",
    "                options_qoute[qoute] = {}\n",
    "                options_qoute[qoute]['start_price'] = df_filter['UNDERLYING_LAST'].values[0]\n",
    "                options_qoute[qoute]['strike'] = df_filter[ df_filter['INTRINSIC_VALUE'].abs().isin(df_filter['INTRINSIC_VALUE'].abs().sort_values()[:max_option_len]) ]['STRIKE'].values\n",
    "                options_qoute[qoute]['exp'] = df_filter['EXPIRE_DATE'].values[0]\n",
    "                #check diff UNDERLYING_LAST\n",
    "                if df_filter['UNDERLYING_LAST'].values[0] != round(np.average(df_filter['UNDERLYING_LAST']),4):\n",
    "                    print('[ERROR] : set UNDERLYING_LAST ',qoute )\n",
    "            #rm index max : max_option_len\n",
    "            #may be initstrike out of scope\n",
    "            rm_strike_index = df_filter[ ~df_filter['STRIKE'].isin(options_qoute[qoute]['strike']) ].index\n",
    "            df_filter = df_filter.drop(rm_strike_index)\n",
    "\n",
    "            #break loops\n",
    "            if len(df_filter) < 5 :  \n",
    "                print(f\"\"\"{msg['qd_ate']}|{msg['symbo']}|{msg['exdate']} : {msg['start_rows']}->break (rm_strike_index)\"\"\")\n",
    "                break\n",
    "                \n",
    "            # Generate zero strike \n",
    "            # Generate a random float between 0.01 and 1\n",
    "            \n",
    "            dte = df_filter[ df_filter['DTE'] != 1e-8]['DTE'].mean()\n",
    "            df_filter['DTE'] = dte\n",
    "            random_number = random.uniform(0, 1)\n",
    "            nom_rm_rows = [r  for r in [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4] if random_number < r]\n",
    "            strikeZero(df_filter,v,len(nom_rm_rows) )\n",
    "            df_filter.loc[df_filter['DTE'] != 0,'TOTAL_VOLUME'] = df_filter[  df_filter['DTE'] != 0  ]['P_VOLUME'].sum() + df_filter[  df_filter['DTE'] != 0  ]['C_VOLUME'].sum()\n",
    "            df_filter.loc[df_filter['DTE'] == 1e-8,'TOTAL_VOLUME'] = 1e-8\n",
    "            hash_id = hash_str(f'{i}{j}')\n",
    "            \n",
    "            #====filed rows\n",
    "            z = 0\n",
    "            while len(df_filter) != max_option_len :\n",
    "                z += 1\n",
    "                if z > 50: raise \"error\"\n",
    "                Even= -1\n",
    "                if (len(df_filter) % 2) == 0:\n",
    "                    Even = 0\n",
    "                if len(df_filter) < max_option_len :\n",
    "                    zero_row = pd.DataFrame([[1e-8]*len(df_filter.columns)], columns=df_filter.columns)\n",
    "                    if Even==0 :\n",
    "                        df_filter = pd.concat([df_filter, zero_row], ignore_index=True)\n",
    "                    else:\n",
    "                        df_filter = pd.concat([zero_row, df_filter], ignore_index=True)\n",
    "                elif len(df_filter) > max_option_len :\n",
    "                     df_filter = df_filter.drop(df_filter.index[Even])\n",
    "            #add cal columns.      \n",
    "            msg['end_rows']= len(df_filter)\n",
    "            if len(df_filter) == max_option_len and not np.isnan( np.sum( df_filter[SCALER_COL].values ) ) :\n",
    "                DATA = np.vstack((DATA ,[df_filter[SCALER_COL]]))\n",
    "            print(f\"\"\"{msg['qd_ate']}|{msg['symbo']}|{msg['exdate']} : {msg['start_rows']}->{msg['end_rows']}  \"\"\")\n",
    "            \n",
    "        if len(DATA) :\n",
    "            clear_output(wait = False)\n",
    "            #========================== partial_fit ================================#\n",
    "            Scaler.partial_fit( DATA.reshape(-1, len(SCALER_COL)) )\n",
    "            joblib.dump(Scaler, \"\".join( ''.join(SCALER_PATH ) ) )\n",
    "            #=== clear expire options_qoute\n",
    "            for qi in list(options_qoute.keys()):\n",
    "                if options_qoute[qi]['exp'] < keys['QUOTE_DATE'].drop_duplicates().values.max():\n",
    "                    options_qoute.pop(qi)\n",
    "        \n",
    "            #========================== SAVE DATA ================================#\n",
    "            #=== save H5\n",
    "            if not os.path.exists(H5_PATH):\n",
    "                os.makedirs(H5_PATH)\n",
    "            with h5py.File(H5_PATH+f\"{partdate}.h5\", 'w') as f:\n",
    "                dset = f.create_dataset(f'{partdate}', data=DATA, chunks=True , compression='gzip')\n",
    "            # #=== save parquet\n",
    "            # if os.path.exists(PARQUET_STG_PATH):\n",
    "            #   df.to_parquet(PARQUET_STG_PATH, engine='fastparquet', append=True, partition_cols=['PartitionDate'], index=False )\n",
    "            # else:\n",
    "            #   df.to_parquet(PARQUET_STG_PATH, engine='fastparquet' , partition_cols=['PartitionDate'], index=False  )   \n",
    "        \n",
    "            print(f\"[Last Processing] {partdate}, {round(((i+1)/len(PartitionDate))*100,2)}%       \")\n",
    "            print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 16, 9), dtype=float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "++++++ PartIII to H5 ++++++ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/OptionsEOD_STG.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mcopy_on_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,partdate \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(PartitionDate) :\n\u001b[0;32m---> 14\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARQUET_STG_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPartitionDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartdate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_VOLUME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_VOLUME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC_VOLUME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC_VOLUME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/OptionsEOD_STG.parquet'"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "import random\n",
    "from IPython.display import clear_output,display, HTML\n",
    "#load scaler\n",
    "scaler = MinMaxScaler()\n",
    "if not os.path.exists(H5_PATH):\n",
    "    os.makedirs(H5_PATH)\n",
    "PartitionDate = [ d[-7:] for d in  os.listdir(PARQUET_PATH) if 'PartitionDate' in d]\n",
    "random.shuffle(PartitionDate)\n",
    "scaler = joblib.load( ''.join(SCALER_PATH ) )\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "for i,partdate in enumerate(PartitionDate) :\n",
    "    df = pd.read_parquet(PARQUET_STG_PATH,engine='pyarrow'\n",
    "                                 , filters=[('PartitionDate', '=', partdate)]\n",
    "                                )\n",
    "    df['P_VOLUME'] = df['P_VOLUME'].fillna(0)\n",
    "    df['C_VOLUME'] = df['C_VOLUME'].fillna(0)\n",
    "    DATA  = np.empty((0,) + (20,9) ) \n",
    "    for opt_id in np.unique( df[[\"OPTIONS_ID\"]].values):\n",
    "        df_filter  = df[df[\"OPTIONS_ID\"]==opt_id]\n",
    "\n",
    "        if len(df_filter) > 15 :\n",
    "            while len(df_filter) != 20:\n",
    "                Even= -1\n",
    "                if (len(df_filter) % 2) == 0:\n",
    "                    Even = 0\n",
    "                if len(df_filter) < 20 :\n",
    "                    df_filter.loc[len(df_filter) ] = [0]*len(df_filter.columns)\n",
    "                    total_vol = df_filter['TOTAL_VOLUME'].values.mean()\n",
    "                    df_filter.loc[:, 'TOTAL_VOLUME'] = total_vol\n",
    "                    df_filter.loc[:, 'OPTIONS_ID']= opt_id\n",
    "                elif len(df_filter) > 20 :\n",
    "                     df_filter = df_filter.drop(df_filter.index[Even])\n",
    "\n",
    "        \n",
    "        if len(df_filter) == 20 and not np.isnan( np.sum( df_filter[SCALER_COL].values ) ) :\n",
    "            DATA = np.vstack((DATA ,[scaler.transform(df_filter[SCALER_COL])]))\n",
    "                    \n",
    "    # Save the NumPy array to an HDF5 file\n",
    "    with h5py.File(H5_PATH+f\"{partdate}.h5\", 'w') as f:\n",
    "        dset = f.create_dataset(f'{partdate}', data=DATA, chunks=True , compression='gzip')\n",
    "\n",
    "    print(f\"[Processing] {partdate}, {round(((i+1)/len(PartitionDate))*100,2)}%     \",end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3============== sampling show 3==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OptTrainData\t\t\t\t\t   OptionsEOD.parquet   scaler\n",
      "'Option Chain Field Definitions - optionsDX.pdf'   archive\n",
      " OptionsEOD.csv\t\t\t\t\t   archive.zip\n"
     ]
    }
   ],
   "source": [
    "!cd data && ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assume X is your data with multiple columns\n",
    "# Specify custom scaling ranges for each column\n",
    "scaling_ranges = {\n",
    "    'column1': (0, 1000),  # Example: column1 has values between 0 and 1000\n",
    "    'column2': (1e-04, 0.1),  # Example: column2 has values between 1e-04 and 0.1\n",
    "    'column3': (0, 1),  # Example: column3 has values between 0 and 1\n",
    "    # Add more columns as needed\n",
    "}\n",
    "\n",
    "# Initialize MinMaxScaler with custom scaling ranges\n",
    "scalers = {}\n",
    "for column, (min_val, max_val) in scaling_ranges.items():\n",
    "    scaler = MinMaxScaler(feature_range=(min_val, max_val))\n",
    "    scaler.partial_fit(X[[column]])  # Partial fit on each column individually\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# Transform each column using its respective scaler\n",
    "X_scaled = np.empty_like(X)\n",
    "for column, scaler in scalers.items():\n",
    "    X_scaled[:, X.columns.get_loc(column)] = scaler.transform(X[[column]])\n",
    "\n",
    "# Use X_scaled for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "=========================================== TEST =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f77a7efb8cf15d18a0cd6bbc71a8985efbc57e2467f435a53ada42728ce0a69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
