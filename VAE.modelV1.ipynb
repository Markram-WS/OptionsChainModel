{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/generative/vae/\n",
    "#https://keras.io/examples/generative/molecule_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install --upgrade pip\n",
    "# !pip install pydot\n",
    "# !apt-get install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 15:05:52.289428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os,shutil,random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output,display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from src.model import Sampling\n",
    "#================== initialization ==================\n",
    "currentTM=dt.datetime.now().strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "PROJECT = \"testVAEModel\"\n",
    "LATENT_DIM = 32\n",
    "VAE_LR = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "PARQUET_PATH = './data/OptionsEOD_STG.parquet'\n",
    "SCALER_PATH = './data/scaler/scaler.gz'\n",
    "UNIQUE_KEYS = ['QUOTE_DATE','SYMBOL','EXPIRE_DATE']\n",
    "SCALER_COL  = ['DTE','INTRINSIC_VALUE', 'TOTAL_VOLUME',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK',\t'P_VOLUME' ]\n",
    "MODEL_PATH = \"./models/\"\n",
    "H5_PATH = './data/OptTrainData/'\n",
    "DISPLAY = False\n",
    "WANDB_LOG = True\n",
    "RESUME = False\n",
    "\n",
    "Scaler = joblib.load(SCALER_PATH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwasan-sinlapa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/workspace/OptionsChainModel/wandb/run-20240711_150556-dd4s69na</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wasan-sinlapa/testVAEModel/runs/dd4s69na/workspace' target=\"_blank\">2024-07-11T150554</a></strong> to <a href='https://wandb.ai/wasan-sinlapa/testVAEModel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wasan-sinlapa/testVAEModel' target=\"_blank\">https://wandb.ai/wasan-sinlapa/testVAEModel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wasan-sinlapa/testVAEModel/runs/dd4s69na/workspace' target=\"_blank\">https://wandb.ai/wasan-sinlapa/testVAEModel/runs/dd4s69na/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notes = f\"\"\"\n",
    "test Run use_bias set false , no tranfrom\n",
    "\"\"\"\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "CONFIG = {\"latent_dim\":LATENT_DIM,\n",
    "          \"learning_rate\": VAE_LR,\n",
    "          \"epochs\": EPOCHS,\n",
    "          \"batch_size\": BATCH_SIZE,\n",
    "          \"architecture\": \"VAE\",\n",
    "          \"dataset\": \"OptionsChaine\",\n",
    "          \"encoder_dense_units\":[128,64],\n",
    "          \"encoder_dropout_rate\":0.2,\n",
    "          \"decoder_dense_units\":[64, 128],\n",
    "          \"decoder_dropout_rate\":0.2,\n",
    "          \"use_bias\":False,\n",
    "          \"transform\":True\n",
    "           }\n",
    "\n",
    "if WANDB_LOG :\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=PROJECT, \n",
    "                     name=currentTM, \n",
    "                     config=CONFIG,\n",
    "                     notes=notes\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "# from IPython.display import clear_output,display, HTML\n",
    "# import numpy as np\n",
    "# #load scaler\n",
    "# scaler = MinMaxScaler()\n",
    "# PartitionDate = [ d[-7:] for d in  os.listdir(PARQUET_PATH) if 'PartitionDate' in d]\n",
    "# random.shuffle(PartitionDate)\n",
    "# scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "\n",
    "# for i,partdate in enumerate(PartitionDate) :\n",
    "#     df = pd.read_parquet(PARQUET_PATH,engine='pyarrow'\n",
    "#                                  , filters=[('PartitionDate', '=', partdate)]\n",
    "#                                 )\n",
    "#     df['P_VOLUME'] = df['P_VOLUME'].fillna(0)\n",
    "#     df['C_VOLUME'] = df['C_VOLUME'].fillna(0)\n",
    "#     DATA  = np.empty((0,) + (20,9) ) \n",
    "#     for opt_id in np.unique( df[[\"OPTIONS_ID\"]].values):\n",
    "#         df_filter  = df[df[\"OPTIONS_ID\"]==opt_id]\n",
    "#         if len(df_filter) == 20:\n",
    "#             DATA = np.vstack((DATA ,[scaler.transform(df_filter[SCALER_COL])]))\n",
    "#         else:\n",
    "#             #print( len(df_filter) )\n",
    "#             #display(HTML(df_filter[['STRIKE']+SCALER_COL].to_html()))\n",
    "#             pass\n",
    "            \n",
    "#     ## Save the NumPy array to an HDF5 file\n",
    "#     # with h5py.File(H5_PATH+f\"{partdate}.h5\", 'w') as f:\n",
    "#     #     dset = f.create_dataset(f'{partdate}', data=DATA, chunks=True , compression='gzip')\n",
    "\n",
    "#     print(f\"[Processing] {partdate}, {round(((i+1)/len(PartitionDate))*100,2)}%     \",end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the NumPy array to an HDF5 file\n",
    "# with h5py.File(H5_PATH, 'w') as f:\n",
    "#     #dset = f.create_dataset('dataset', data=DATA, chunks=True, compression='gzip')\n",
    "#     #test\n",
    "#     dset = f.create_dataset('dataset', data=DATA, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALER_COL  = ['DTE','INTRINSIC_VALUE', 'TOTAL_VOLUME',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK',\t'P_VOLUME' ]\n",
    "select_x = [i for i,c in  enumerate(SCALER_COL) if c in ['DTE','INTRINSIC_VALUE'] ]\n",
    "select_y = [i for i,c in enumerate(SCALER_COL) if c in ['C_BID',\t'C_ASK',  'P_BID',\t'P_ASK'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import OptionChainGenerator\n",
    "from src.layer import encoder, decoder\n",
    "\n",
    "model = OptionChainGenerator(\n",
    "    encoder(latent_dim = LATENT_DIM, \n",
    "            input_shape= (32,len(select_x) ), \n",
    "            dense_units = CONFIG[\"encoder_dense_units\"], \n",
    "            dropout_rate= CONFIG[\"encoder_dropout_rate\"],\n",
    "            use_bias=CONFIG[\"use_bias\"]\n",
    "           ), \n",
    "    decoder(latent_dim  = LATENT_DIM , \n",
    "            output_shape= (32,len(select_y) ),\n",
    "            dense_units = CONFIG[\"decoder_dense_units\"],\n",
    "            dropout_rate= CONFIG[\"decoder_dropout_rate\"],\n",
    "            use_bias=CONFIG[\"use_bias\"]\n",
    "           )\n",
    ")\n",
    "\n",
    "def dummy_loss(y_true, y_pred):\n",
    "    return 0.0\n",
    "    \n",
    "vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)\n",
    "model.compile(vae_optimizer )#, loss=dummy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## show model ######################\n",
    "if DISPLAY :\n",
    "    from tensorflow.keras.utils import model_to_dot\n",
    "    from IPython.display import SVG, display\n",
    "    \n",
    "    def display_model(model, width=1024, height=512):\n",
    "        dot = model_to_dot(model, show_shapes=True, show_layer_names=True)\n",
    "        svg_data = dot.create(prog='dot', format='svg').decode(\"utf-8\")\n",
    "        svg_html = f'<div style=\"width:{width}px;height:{height}px;\">{svg_data}</div>'\n",
    "        display(HTML(svg_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage:\n",
    "## Display the encoder model with reduced size\n",
    "if DISPLAY :\n",
    "    display_model(model.encoder, width=1024, height=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISPLAY :\n",
    "    display_model(model.decoder, width=2500, height=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== loadmodel ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_path = MODEL_PATH+f'{PROJECT}'\n",
    "if not RESUME :\n",
    "    if os.path.exists(model_path) :\n",
    "        shutil.rmtree(model_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.encoder.save(model_path+f'/'+f'encoder.keras') \n",
    "    model.decoder.save(model_path+f'/'+f'decoder.keras') \n",
    "else:\n",
    "    model.encoder = load_model(model_path+'/'+f'encoder.keras') \n",
    "    model.decoder = load_model(model_path+'/'+f'decoder.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - kl_loss: 3.7786e-06 - total_loss: 1.7322 - val_kl_loss: 2.4195e-08 - val_total_loss: 1.5279\n",
      "Epoch 2/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - kl_loss: 1.5650e-05 - total_loss: 1.7991 - val_kl_loss: 8.7558e-05 - val_total_loss: 1.5253\n",
      "Epoch 3/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - kl_loss: 1.2766e-04 - total_loss: 1.7670 - val_kl_loss: 8.4603e-05 - val_total_loss: 1.5191\n",
      "Epoch 4/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - kl_loss: 1.1371e-04 - total_loss: 1.6534 - val_kl_loss: 1.5133e-04 - val_total_loss: 1.5235\n",
      "Epoch 5/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - kl_loss: 3.1321e-04 - total_loss: 1.5607 - val_kl_loss: 1.8881e-04 - val_total_loss: 1.5221\n",
      "Epoch 6/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - kl_loss: 2.1626e-04 - total_loss: 1.6680 - val_kl_loss: 2.0927e-04 - val_total_loss: 1.5130\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#================== train model ==================\n",
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "\n",
    "STOP_MODEL = False\n",
    "STACK_DATA = np.empty((0,) + (32,9) ) #init STACK_DATA\n",
    "\n",
    "for partdate in PartitionDate[:] :\n",
    "    clear_output(wait=False)\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "    data_shape = DATA.shape\n",
    "    ###transform\n",
    "    if CONFIG['transform'] :\n",
    "        DATA = Scaler.transform(DATA.reshape(-1,data_shape[-1]))\n",
    "        DATA = DATA.reshape(data_shape)\n",
    "    DATA = np.vstack((DATA ,STACK_DATA))\n",
    "    if len(DATA) < 64 :\n",
    "        #stack data\n",
    "        STACK_DATA = np.vstack((STACK_DATA ,DATA))\n",
    "    else: \n",
    "        # if DATA.isna().sum().sum() > 0:\n",
    "        #     print(\"Data contains NaNs. Please handle them before scaling.\")\n",
    "        STACK_DATA = np.empty((0,) + (32,9) )\n",
    "        X = DATA[:, :, select_x]  # เลือกข้อมูลแถวแรกถึงแถวที่ 3 สำหรับ X\n",
    "        Y = DATA[:, :, select_y]  # เลือกข้อมูลแถวที่ 3 เป็นต้นไปสำหรับ Y\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        random.shuffle(PartitionDate)\n",
    "        tf.keras.backend.clear_session() \n",
    "        history = model.fit(x_train , y_train, epochs=CONFIG['epochs'], batch_size=BATCH_SIZE, validation_data=(x_val, y_val) )\n",
    "        #history = model.fit(x_train , y_train, epochs=5, batch_size=BATCH_SIZE )\n",
    "        if  np.isnan(  np.average( history.history['kl_loss'] )  ) or np.isnan(  np.average( history.history['val_kl_loss'] )  ):\n",
    "            STOP_MODEL = True \n",
    "            print(x_train)\n",
    "            print(\"---\")\n",
    "            print(x_val)\n",
    "            print(\"=============\")\n",
    "        if WANDB_LOG :\n",
    "            LogKeys = history.history.keys()\n",
    "            LogVal={}\n",
    "            for k in LogKeys:  \n",
    "                LogVal[k] = np.average(  history.history[k] )\n",
    "            wandb.log(LogVal, commit=True)\n",
    "        \n",
    "    if STOP_MODEL :\n",
    "        break\n",
    "    \n",
    "            \n",
    "    model.encoder.save(model_path+f'/'+f'encoder.keras') \n",
    "    model.decoder.save(model_path+f'/'+f'decoder.keras') \n",
    "if WANDB_LOG : wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================== predict ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PartitionDate \u001b[38;5;241m=\u001b[39m [ d[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m  \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(H5_PATH)]\n\u001b[1;32m      2\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(PartitionDate)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m partdate \u001b[38;5;129;01min\u001b[39;00m PartitionDate[:\u001b[38;5;241m1\u001b[39m] :\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "PartitionDate = [ d[:-3] for d in  os.listdir(H5_PATH)]\n",
    "random.shuffle(PartitionDate)\n",
    "for partdate in PartitionDate[:1] :\n",
    "    DATA = []\n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "    data_shape = DATA.shape\n",
    "    if CONFIG['transform'] :\n",
    "        DATA_TF = Scaler.transform(DATA.reshape(-1,data_shape[-1]))\n",
    "        DATA_TF = DATA_TF.reshape(data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DATA_TF[:, :, select_x][:1]\n",
    "Y_real = DATA_TF[:, :, select_y][:1]\n",
    "# X data\n",
    "df = pd.DataFrame(\n",
    "    DATA[:, :, :3][:1].reshape(32, 3), \n",
    "    columns=SCALER_COL[:3])\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m z_mean, log_var \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mencoder(X) \n\u001b[1;32m      2\u001b[0m z \u001b[38;5;241m=\u001b[39m Sampling()([z_mean, log_var])\n\u001b[1;32m      3\u001b[0m decode_data \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(z)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "z_mean, log_var = model.encoder(X) \n",
    "z = Sampling()([z_mean, log_var])\n",
    "decode_data = model.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32, 4), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.7234566 , 0.72337925, 0.7233077 , 0.7233854 ],\n",
       "        [0.689044  , 0.68909174, 0.6890693 , 0.6890581 ],\n",
       "        [0.6689366 , 0.66889364, 0.668935  , 0.6688881 ],\n",
       "        [0.78139985, 0.781265  , 0.7814391 , 0.7812704 ],\n",
       "        [0.7381041 , 0.73812735, 0.7379848 , 0.7379633 ],\n",
       "        [0.69196725, 0.69200623, 0.6919941 , 0.69195676],\n",
       "        [0.6634506 , 0.6632937 , 0.66315794, 0.66328907],\n",
       "        [0.6242391 , 0.6240769 , 0.6243938 , 0.62421733],\n",
       "        [0.5877602 , 0.5886376 , 0.5882968 , 0.58698463],\n",
       "        [0.5496805 , 0.5484245 , 0.55076563, 0.54961896],\n",
       "        [0.5212235 , 0.        , 0.5222677 , 0.5223665 ],\n",
       "        [0.51650155, 0.51711416, 0.5165999 , 0.5171455 ],\n",
       "        [0.45536894, 0.        , 0.45721126, 0.45691222],\n",
       "        [0.41686386, 0.41421676, 0.42029712, 0.41843235],\n",
       "        [0.18158126, 0.30093598, 0.35172415, 0.1742527 ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.40191274, -0.40153765, -0.3737459 , -0.24608101],\n",
       "        [-0.40470174, -0.404874  , -0.3737459 , -0.24594222],\n",
       "        [-0.40825137, -0.40790143, -0.3737459 , -0.24594222],\n",
       "        [-0.41154746, -0.41074351, -0.37352711, -0.24594222],\n",
       "        [-0.41389275, -0.41401808, -0.3736365 , -0.24587282],\n",
       "        [-0.41699868, -0.4171073 , -0.37352711, -0.24594222],\n",
       "        [-0.42035815, -0.42013473, -0.37341772, -0.24587282],\n",
       "        [-0.42308376, -0.42322395, -0.37308954, -0.24559523],\n",
       "        [-0.42599953, -0.42594246, -0.37276136, -0.24531765],\n",
       "        [-0.42859837, -0.42859919, -0.37166743, -0.24469308],\n",
       "        [-0.43081689, -0.43076164, -0.37013594, -0.24365213],\n",
       "        [-0.43252832, -0.43255338, -0.3677293 , -0.2421254 ],\n",
       "        [-0.43373266, -0.43372729, -0.36444751, -0.2401129 ],\n",
       "        [-0.4344933 , -0.43440691, -0.3601812 , -0.23733703],\n",
       "        [-0.43468346, -0.43459227, -0.35525852, -0.23421418],\n",
       "        [-0.43487361, -0.43477762, -0.35022646, -0.23074435],\n",
       "        [-0.434937  , -0.43490119, -0.345085  , -0.22734391],\n",
       "        [-0.434937  , -0.43490119, -0.3405999 , -0.22283312],\n",
       "        [-0.434937  , -0.43490119, -0.33502087, -0.2192245 ],\n",
       "        [-0.434937  , -0.4348394 , -0.33009819, -0.21575466],\n",
       "        [-0.434937  , -0.4348394 , -0.32451916, -0.21228483],\n",
       "        [-0.434937  , -0.4348394 , -0.31904953, -0.20888439],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101],\n",
       "        [-0.434937  , -0.43490119, -0.3737459 , -0.24608101]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= _compute_loss =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated_data = [c_bid, c_ask, c_volume, p_bid, p_ask, p_volume]\n",
    "colList = [\"c_bid\", \"c_ask\", \"c_volume\", \"p_bid\", \"p_ask\", \"p_volume\"]\n",
    "generated_data = decode_data[3:]\n",
    "z_mean    = z_mean\n",
    "z_log_var = log_var\n",
    "Y_real    = DATA[:, :, 3:][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  col,genData in zip(colList,generated_data):\n",
    "    print( colList.index(col),col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_genData = genData - tf.cast(tf.expand_dims(Y_real[:, :, colList.index(col)], axis=-1)\n",
    "        , tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_values_total = []\n",
    "reconstruction_values_total.append( tf.reduce_mean( tf.square(subtract_genData)   ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_var = tf.clip_by_value(log_var, -1.0, 1.0)\n",
    "kl_loss = -0.5 * tf.reduce_sum(1 + log_var - tf.square(z_mean) - tf.exp(log_var), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(reconstruction_values_total + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "========== kiras vae origi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.concat(decode_data, axis=-1).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real[0] -  tf.concat(decode_data, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loss = tf.reduce_mean(\n",
    "    tf.reduce_sum(\n",
    "        tf.keras.losses.mean_squared_error(Y_real, tf.concat(decode_data, axis=-1)),\n",
    "        axis=(1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_loss = tf.reduce_mean(\n",
    "#     tf.reduce_sum(\n",
    "#         keras.losses.categorical_crossentropy(features_real, features_gen),\n",
    "#         axis=(1),\n",
    "#     )\n",
    "# )\n",
    "# kl_loss = -0.5 * tf.reduce_sum(\n",
    "#     1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1\n",
    "# )\n",
    "# kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "# property_loss = tf.reduce_mean(\n",
    "#     keras.losses.binary_crossentropy(qed_true, qed_pred)\n",
    "# )\n",
    "\n",
    "# graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
    "\n",
    "# return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "======================= inverse_transform ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 0\n",
    "decode_data = [tf.zeros([1, 32, 1])]*3 + decode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_decode = Scaler.inverse_transform(\n",
    "    np.array([d.numpy().reshape(-1) for d in decode_data]).transpose()\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    invert_decode[:,3:], \n",
    "    columns=SCALER_COL[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f77a7efb8cf15d18a0cd6bbc71a8985efbc57e2467f435a53ada42728ce0a69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
