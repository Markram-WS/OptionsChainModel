{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/generative/vae/\n",
    "#https://keras.io/examples/generative/molecule_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade pip\n",
    "#!pip install pydot\n",
    "#!apt-get install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 08:53:50.421002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os,shutil,random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output,display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "#================== initialization ==================\n",
    "currentTM=dt.datetime.now().strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "PROJECT = \"testVAEModel\"\n",
    "LATENT_DIM = 32\n",
    "VAE_LR = 1e-5\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "PARQUET_PATH = './data/OptionsEOD_STG.parquet'\n",
    "SCALER_PATH = './data/scaler.gz'\n",
    "UNIQUE_KEYS = ['QUOTE_DATE','SYMBOL','EXPIRE_DATE']\n",
    "SCALER_COL  = ['DTE','INTRINSIC_VALUE', 'TOTAL_VOLUME',\t'C_BID',\t'C_ASK', 'C_VOLUME',  'P_BID',\t'P_ASK',\t'P_VOLUME' ]\n",
    "MODEL_PATH = \"./models/\"\n",
    "H5_PATH = './data/OptTrainData/'\n",
    "DISPLAY = False\n",
    "WANDB_LOG = True\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwasan-sinlapa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/workspace/OptionsChainModel/wandb/run-20240705_085354-9y5nfnan</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wasan-sinlapa/testVAEModel/runs/9y5nfnan/workspace' target=\"_blank\">2024-07-05T085352</a></strong> to <a href='https://wandb.ai/wasan-sinlapa/testVAEModel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wasan-sinlapa/testVAEModel' target=\"_blank\">https://wandb.ai/wasan-sinlapa/testVAEModel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wasan-sinlapa/testVAEModel/runs/9y5nfnan/workspace' target=\"_blank\">https://wandb.ai/wasan-sinlapa/testVAEModel/runs/9y5nfnan/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#741d760b304d0be5b18d4ee9682f77156e6967b5\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "CONFIG = {    \"latent_dim\":LATENT_DIM,\n",
    "                  \"learning_rate\": VAE_LR,\n",
    "                  \"epochs\": EPOCHS,\n",
    "                  \"batch_size\": BATCH_SIZE,\n",
    "                  \"architecture\": \"VAE\",\n",
    "                  \"dataset\": \"OptionsChaine\",\n",
    "                  \"encoder_dense_units\":[128,64],\n",
    "                  \"encoder_dropout_rate\":0.2,\n",
    "                  \"decoder_dense_units\":[64, 128],\n",
    "                  \"decoder_dropout_rate\":0.2,\n",
    "               }\n",
    "    \n",
    "if WANDB_LOG :\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=PROJECT, name=currentTM, config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "# from IPython.display import clear_output,display, HTML\n",
    "# import numpy as np\n",
    "# #load scaler\n",
    "# scaler = MinMaxScaler()\n",
    "# PartitionDate = [ d[-7:] for d in  os.listdir(PARQUET_PATH) if 'PartitionDate' in d]\n",
    "# random.shuffle(PartitionDate)\n",
    "# scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "\n",
    "# for i,partdate in enumerate(PartitionDate) :\n",
    "#     df = pd.read_parquet(PARQUET_PATH,engine='pyarrow'\n",
    "#                                  , filters=[('PartitionDate', '=', partdate)]\n",
    "#                                 )\n",
    "#     df['P_VOLUME'] = df['P_VOLUME'].fillna(0)\n",
    "#     df['C_VOLUME'] = df['C_VOLUME'].fillna(0)\n",
    "#     DATA  = np.empty((0,) + (20,9) ) \n",
    "#     for opt_id in np.unique( df[[\"OPTIONS_ID\"]].values):\n",
    "#         df_filter  = df[df[\"OPTIONS_ID\"]==opt_id]\n",
    "#         if len(df_filter) == 20:\n",
    "#             DATA = np.vstack((DATA ,[scaler.transform(df_filter[SCALER_COL])]))\n",
    "#         else:\n",
    "#             #print( len(df_filter) )\n",
    "#             #display(HTML(df_filter[['STRIKE']+SCALER_COL].to_html()))\n",
    "#             pass\n",
    "            \n",
    "#     ## Save the NumPy array to an HDF5 file\n",
    "#     # with h5py.File(H5_PATH+f\"{partdate}.h5\", 'w') as f:\n",
    "#     #     dset = f.create_dataset(f'{partdate}', data=DATA, chunks=True , compression='gzip')\n",
    "\n",
    "#     print(f\"[Processing] {partdate}, {round(((i+1)/len(PartitionDate))*100,2)}%     \",end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the NumPy array to an HDF5 file\n",
    "# with h5py.File(H5_PATH, 'w') as f:\n",
    "#     #dset = f.create_dataset('dataset', data=DATA, chunks=True, compression='gzip')\n",
    "#     #test\n",
    "#     dset = f.create_dataset('dataset', data=DATA, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import OptionChainGenerator\n",
    "from src.layer import encoder, decoder\n",
    "\n",
    "model = OptionChainGenerator(\n",
    "    encoder(latent_dim = LATENT_DIM, \n",
    "            input_shape= (20,3), \n",
    "            dense_units = CONFIG[\"encoder_dense_units\"], \n",
    "            dropout_rate= CONFIG[\"encoder_dropout_rate\"]\n",
    "           ), \n",
    "    decoder(latent_dim  = LATENT_DIM , \n",
    "            output_shape= (20,1),\n",
    "            dense_units = CONFIG[\"decoder_dense_units\"],\n",
    "            dropout_rate= CONFIG[\"decoder_dropout_rate\"]\n",
    "           )\n",
    ")\n",
    "\n",
    "def dummy_loss(y_true, y_pred):\n",
    "    return 0.0\n",
    "    \n",
    "vae_optimizer = tf.keras.optimizers.Adam(learning_rate=VAE_LR)\n",
    "model.compile(vae_optimizer )#, loss=dummy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## show model ######################\n",
    "if DISPLAY :\n",
    "    from tensorflow.keras.utils import model_to_dot\n",
    "    from IPython.display import SVG, display\n",
    "    \n",
    "    def display_model(model, width=1024, height=512):\n",
    "        dot = model_to_dot(model, show_shapes=True, show_layer_names=True)\n",
    "        svg_data = dot.create(prog='dot', format='svg').decode(\"utf-8\")\n",
    "        svg_html = f'<div style=\"width:{width}px;height:{height}px;\">{svg_data}</div>'\n",
    "        display(HTML(svg_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage:\n",
    "## Display the encoder model with reduced size\n",
    "if DISPLAY :\n",
    "    display_model(model.encoder, width=1024, height=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISPLAY :\n",
    "    display_model(model.decoder, width=2500, height=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== loadmodel ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_path = MODEL_PATH+f'{PROJECT}'\n",
    "if not RESUME :\n",
    "    if os.path.exists(model_path) :\n",
    "        shutil.rmtree(model_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.encoder.save(model_path+f'/'+f'encoder.keras') \n",
    "    model.decoder.save(model_path+f'/'+f'decoder.keras') \n",
    "else:\n",
    "    model.encoder = load_model(model_path+'/'+f'encoder.keras') \n",
    "    model.decoder = load_model(model_path+'/'+f'decoder.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - kl_loss: 1.5858e-09 - total_loss: 0.0023 - vol_loss: 0.0036 - val_kl_loss: 0.0000e+00 - val_total_loss: 0.0033 - val_vol_loss: 0.0021\n",
      "Epoch 2/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - kl_loss: 0.0000e+00 - total_loss: 0.0024 - vol_loss: 0.0035 - val_kl_loss: 0.0000e+00 - val_total_loss: 0.0033 - val_vol_loss: 0.0023\n",
      "Epoch 3/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - kl_loss: 0.0000e+00 - total_loss: 0.0023 - vol_loss: 0.0037 - val_kl_loss: 0.0000e+00 - val_total_loss: 0.0033 - val_vol_loss: 0.0021\n",
      "Epoch 4/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - kl_loss: 0.0000e+00 - total_loss: 0.0021 - vol_loss: 0.0036 - val_kl_loss: 0.0000e+00 - val_total_loss: 0.0033 - val_vol_loss: 0.0023\n",
      "Epoch 5/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - kl_loss: 4.3120e-09 - total_loss: 0.0023 - vol_loss: 0.0037 - val_kl_loss: 0.0000e+00 - val_total_loss: 0.0033 - val_vol_loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>kl_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_kl_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_total_loss</td><td>█▅▃▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>val_vol_loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>vol_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>kl_loss</td><td>0.0</td></tr><tr><td>total_loss</td><td>0.0</td></tr><tr><td>val_kl_loss</td><td>0.0</td></tr><tr><td>val_total_loss</td><td>0.00327</td></tr><tr><td>val_vol_loss</td><td>0.0022</td></tr><tr><td>vol_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-07-05T085352</strong> at: <a href='https://wandb.ai/wasan-sinlapa/testVAEModel/runs/9y5nfnan/workspace' target=\"_blank\">https://wandb.ai/wasan-sinlapa/testVAEModel/runs/9y5nfnan/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_085354-9y5nfnan/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#================== train model ==================\n",
    "PartitionDate = [ d[-7:] for d in  os.listdir(PARQUET_PATH) if 'PartitionDate' in d]\n",
    "random.shuffle(PartitionDate)\n",
    "\n",
    "STOP_MODEL = False\n",
    "for partdate in PartitionDate[:] :\n",
    "    clear_output(wait=False)\n",
    "    #nan problem\n",
    "    #partdate = '2022-05'\n",
    "    #normal \n",
    "    #partdate = '2011-12'\n",
    "    \n",
    "    with h5py.File(H5_PATH+partdate+\".h5\", 'r') as f:\n",
    "        DATA = f[partdate][:]\n",
    "        X = DATA[:, :, :3]  # เลือกข้อมูลแถวแรกถึงแถวที่ 3 สำหรับ X\n",
    "        Y = DATA[:, :, 3:]  # เลือกข้อมูลแถวที่ 3 เป็นต้นไปสำหรับ Y\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        if len(X) :\n",
    "            random.shuffle(PartitionDate)\n",
    "            tf.keras.backend.clear_session() \n",
    "            history = model.fit(x_train , y_train, epochs=5, batch_size=BATCH_SIZE, validation_data=(x_val, y_val) )\n",
    "            if  np.isnan(  np.average( history.history['kl_loss'] )  ) or np.isnan(  np.average( history.history['val_kl_loss'] )  ):\n",
    "                STOP_MODEL = True \n",
    "                print(x_train)\n",
    "                print(\"---\")\n",
    "                print(x_val)\n",
    "                print(\"=============\")\n",
    "    if WANDB_LOG :\n",
    "        LogKeys = history.history.keys()\n",
    "        LogVal={}\n",
    "        for k in LogKeys:  \n",
    "            LogVal[k] = np.average(  history.history[k] )\n",
    "        wandb.log(LogVal, commit=True)\n",
    "        \n",
    "    if STOP_MODEL :\n",
    "        break\n",
    "    \n",
    "            \n",
    "    model.encoder.save(model_path+f'/'+f'encoder.keras') \n",
    "    model.decoder.save(model_path+f'/'+f'decoder.keras') \n",
    "if WANDB_LOG : wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2094489582.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    `====================================================\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "`===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.94499003e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.94241586e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.93984169e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.93726752e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.93469335e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.93211918e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.92954502e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.92697085e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.92439668e-01, 1.73585972e-04],\n",
       "       [1.79048797e-01, 3.92182251e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04],\n",
       "       [0.00000000e+00, 3.92071562e-01, 1.73585972e-04]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.expand_dims(y_val[:, :, 0], axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a:\n",
    "    if np.isnan( np.sum(i) ) :\n",
    "        print(i)\n",
    "        print('-------')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_val)\n",
    "print(\"---\")\n",
    "print(y_val)\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=y_val\n",
    "for i in range ( len( np.transpose( Y[0] ) ) ):\n",
    "    col = np.transpose( Y[i] ) \n",
    "    for c in range(len(col)):\n",
    "        a = np.transpose( Y[i] )[c]\n",
    "        if np.sum( a ) == 0 :\n",
    "            print(f\"0 - i:{i},c:{c}\")\n",
    "        if np.isnan( np.sum( a ) ) :\n",
    "            print(f\"nan - i:{i},c:{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f77a7efb8cf15d18a0cd6bbc71a8985efbc57e2467f435a53ada42728ce0a69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
